{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Hashes and Scores\n",
    "\n",
    "- Load in a set of hashes and distance scores that have already been calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "from joblib import load\n",
    "from phaser.utils import load_labelencoders\n",
    "\n",
    "\n",
    "hash_dist_dir = r\"demo_outputs\"\n",
    "\n",
    "# Load the label encoders\n",
    "le = load_labelencoders(filename=\"LabelEncoders.bz2\", path=hash_dist_dir)\n",
    "\n",
    "# Get values to construct triplets\n",
    "TRANSFORMS = le[\"t\"].classes_\n",
    "METRICS = le[\"m\"].classes_\n",
    "ALGORITHMS = le[\"a\"].classes_\n",
    "\n",
    "# Load from CSV\n",
    "#df_h = pd.read_csv(os.path.join(hash_dist_dir , \"Hashes.csv.bz2\"))\n",
    "#df_d = pd.read_csv(os.path.join(hash_dist_dir , \"Distances.csv.bz2\"))\n",
    "\n",
    "# Load from the df files instead (a better option for larger datasets)\n",
    "df_h = load(os.path.join(hash_dist_dir , \"Hashes.df.bz2\"))\n",
    "df_d = load(os.path.join(hash_dist_dir , \"Distances.df.bz2\"))\n",
    "\n",
    "# Inter (0), Intra (1)\n",
    "intra_df = df_d[df_d[\"class\"] == 1]\n",
    "inter_df = df_d[df_d[\"class\"] == 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inter-Score Averages\n",
    "\n",
    "Get a quick idea of the raw inter-distance / inter-similarity score distributions for each algorithm and metric.\n",
    "\n",
    "This gives us an idea of whether or not they are biased somehow. \n",
    "A good metric should be about **0.5** for both mean and median (normally distributed around 0.5), ideally with a narrow spread. Though, in practice, anything is potentially fine as long as the inter-score and intra-score distributions are separable.\n",
    "\n",
    "\n",
    "Note that values going forward are for SIMILARITY rather than distance unless otherwise specified, as that's the default used in the scientific libraries. We can convert from one to the other by subtracting values from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"====SIMILARITY Values====\")\n",
    "\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Hash\", \"Metric\", \"Mean\", \"Median\", \"STD\"]\n",
    "# Set all columns to left alignment\n",
    "for field in table.field_names:\n",
    "    table.align[field] = \"l\"\n",
    "\n",
    "for a in ALGORITHMS:\n",
    "    for m in METRICS:\n",
    "            m_label = le[\"m\"].transform(np.array(m).ravel())\n",
    "            a_label = le[\"a\"].transform(np.array(a).ravel())\n",
    "            _X = inter_df.query(f\"algo=={a_label} and metric == {m_label}\")[\"orig\"].values\n",
    "            table.add_row([a, m, round(np.mean(_X), 4), round(np.median(_X), 4), round(np.std(_X), 4)])\n",
    "print(table)\n",
    "\n",
    "            \n",
    "print(\"\\n====DISTANCE Values====\")\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Hash\", \"Metric\", \"Mean\", \"Median\", \"STD\"]\n",
    "# Set all columns to left alignment\n",
    "for field in table.field_names:\n",
    "    table.align[field] = \"l\"\n",
    "for a in ALGORITHMS:\n",
    "    for m in METRICS:\n",
    "            m_label = le[\"m\"].transform(np.array(m).ravel())\n",
    "            a_label = le[\"a\"].transform(np.array(a).ravel())\n",
    "            _X = inter_df.query(f\"algo=={a_label} and metric == {m_label}\")[\"orig\"].values\n",
    "            table.add_row([a, m, round(np.mean(_X), 4), round(np.median(_X), 4), round(np.std(_X), 4)])\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Metrics\n",
    "\n",
    "- Generate triplet permutations: **Perceptual Algorithm** -- **Transform** -- **Distance Metric**\n",
    "- Generate additional stats (ComputeMetrics class), save to **CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phaser.evaluation import ComputeMetrics\n",
    "\n",
    "# Define the triplet combinations\n",
    "triplets = np.array(np.meshgrid(\n",
    "    ALGORITHMS, \n",
    "    [t for t in TRANSFORMS if t != 'orig'], # ignore 'orig'\n",
    "    METRICS)).T.reshape(-1,3)\n",
    "cm = ComputeMetrics(le, df_d, pd.DataFrame(), analyse_bits=False, n_jobs=4)\n",
    "m, b = cm.fit(triplets, weighted=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Save stats to file, print some here\n",
    "\n",
    "print(\"Saving stat files.\")\n",
    "# Dump entire stats dataframe to file\n",
    "m.to_csv(os.path.join(hash_dist_dir, \"all_stats.csv\"))\n",
    "# Separately dump a handy AUC aggregation for all transforms in a metric/algorithm pair\n",
    "m.groupby(['Algorithm', \"Metric\"])[['AUC', \"FP\", \"FN\", \"TP\", \"TN\"]].agg(['mean','std']).to_csv(os.path.join(hash_dist_dir, \"aggregate_AUC.csv\"))\n",
    "\n",
    "\n",
    "\n",
    "# # AUC info\n",
    "print(m.groupby(['Algorithm', \"Metric\"])[['AUC']].agg(['mean','std']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC Heatmaps\n",
    "- Visualise Area under the ROC Curve (AUC) for each Triplet as a heatmap.\n",
    "- If there is more than one distance metric, facilitate a comparison between them and a chosen baseline (default to Hamming) as a heatmap\n",
    "    - Note this is presented as percentage point (%pt) change vs. baseline, so 3.5 would mean 0.035 change in the AUC value from the first heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interactive\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "\n",
    "### Interactive Heatmap Function\n",
    "mselect = widgets.Dropdown(\n",
    "        options=METRICS,\n",
    "        description='Compare to:',\n",
    "        value=\"Hamming\" if \"Hamming\" in METRICS else METRICS[0]\n",
    "    )\n",
    "\n",
    "# Checkbox for squaring the heatmap\n",
    "checkbox = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Square heatmap',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "def heatmap_all(square, width=12, height=10):\n",
    "    \"\"\"Heatmap of AUCs for all Triplets\"\"\"\n",
    "    \n",
    "    heatmap_data = m.pivot_table(index=['Metric', 'Algorithm'], columns='Transform', values='AUC')\n",
    "    fig, ax = plt.subplots(figsize=(width, height))\n",
    "    ax.set_title(f\"AUC of all Triplets\")\n",
    "    fig = sns.heatmap(heatmap_data, annot=True, cmap='RdBu', center=0, square=square, vmin=0, vmax=1, ax=ax)\n",
    "\n",
    "def heatmap_vs_baseline(baseline_metric, square, width=12, height=10):\n",
    "    \"\"\"Heatmap of differences of all AUCs to a given comparison baseline (usually Hamming Distance)\"\"\"\n",
    "    \n",
    "    mi = m.copy() # avoid messing with the stats as they can take a bit to process.\n",
    "    \n",
    "    # Extract Hamming AUCs\n",
    "    baseline_aucs = mi[mi['Metric'] == baseline_metric].set_index(['Transform', 'Algorithm'])['AUC'].rename(f'{baseline_metric} AUC')\n",
    "\n",
    "    # Merge Hamming AUCs with the original DataFrame\n",
    "    mi = mi.set_index(['Transform', 'Algorithm']).join(baseline_aucs, on=['Transform', 'Algorithm'])\n",
    "\n",
    "    # Calculate the differences\n",
    "    mi['Diff to baseline_metric'] = (mi['AUC'] - mi[f'{baseline_metric} AUC']) * 100\n",
    "    mi['Diff to baseline_metric'] = mi['Diff to baseline_metric'].round(3)\n",
    "\n",
    "    # Filter out Hamming distance rows (since their difference will be zero)\n",
    "    subset_df = mi[mi['Metric'] != baseline_metric]\n",
    "    \n",
    "    #heatmap_data = subset_df.pivot_table(index=['Algorithm', 'Transform'], columns='Metric', values='AUC')\n",
    "    \n",
    "    heatmap_data = subset_df.pivot_table(index=['Metric', 'Algorithm'], columns='Transform', values=f'Diff to baseline_metric')\n",
    "    fig, ax = plt.subplots(figsize=(width, height))\n",
    "    ax.set_title(f\"%pt AUC change vs. {baseline_metric}\")\n",
    "    if baseline_metric != 'Select':\n",
    "        \n",
    "        fig = sns.heatmap(heatmap_data, annot=True, cmap='RdBu', center=0, square=square, vmin=-1, vmax=1, ax=ax)\n",
    "        \n",
    "\n",
    "# Display Interactive Heatmaps\n",
    "\n",
    "map1 = interactive(heatmap_all, square=checkbox)\n",
    "display(map1)\n",
    "\n",
    "# Comparison to baseline - Only shows if there are multiple metrics (otherwise there is no comparison to be made!)\n",
    "if len(METRICS) > 1:\n",
    "    map2 = interactive(heatmap_vs_baseline, baseline_metric=mselect, square=checkbox)\n",
    "    display(map2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Plots\n",
    "\n",
    "- Run this before the plot segments below.\n",
    "- Allows for some configurability and interactivity of several plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phaser.evaluation import MetricMaker\n",
    "from phaser.plotting import  hist_fig, kde_ax, eer_ax, roc_ax\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive\n",
    "import ipywidgets as widgets\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # Ignore Seaborn warnings due to underlying package using future deprecated calls\n",
    "\n",
    "\n",
    "#define widgets\n",
    "tselect = widgets.Dropdown(\n",
    "        options=TRANSFORMS,\n",
    "        description='Transform'\n",
    "    )\n",
    "\n",
    "#define widgets\n",
    "tselect_no_orig = widgets.Dropdown(\n",
    "        options=TRANSFORMS[:-1],\n",
    "        description='Transform'\n",
    "    )\n",
    "\n",
    "mselect = widgets.Dropdown(\n",
    "        options=METRICS,\n",
    "        description='Metric'\n",
    "    )\n",
    "aselect = widgets.Dropdown(\n",
    "        options=ALGORITHMS,\n",
    "        description='Algorithm'\n",
    "    )\n",
    "modeselect = widgets.Dropdown(\n",
    "        options=[\"inter\", \"intra\"],\n",
    "        description='Comparison Mode'\n",
    "    )\n",
    "\n",
    "\n",
    "### Hist plots, separate for intra/inter\n",
    "def plot_image(transform, mode, bins=25,  width=8, height=6):\n",
    "    data = df_h\n",
    "    if transform != 'Select' and bins > 1:\n",
    "        if mode == \"inter\":\n",
    "            fig = hist_fig(inter_df, label_encoding=le, transform=transform, interactive=True, bins=bins, figsize=(width,height))\n",
    "        elif mode == \"intra\":\n",
    "            fig = hist_fig(intra_df, label_encoding=le, transform=transform, interactive=True, bins=bins, figsize=(width,height))\n",
    "        fig.suptitle(f\"Similarity Histograms - {transform}\")\n",
    "        \n",
    "\n",
    "### KDE multi plot\n",
    "def kde_plot_multi(transform, width=8, height=6):\n",
    "    if transform != 'Select':\n",
    "\n",
    "        #t_label = le_a.transform(np.array(transform).ravel()\n",
    "        n_cols = len(METRICS)\n",
    "        n_rows = len(ALGORITHMS)\n",
    "\n",
    "        # Subset data\n",
    "        fig, axes = plt.subplots(ncols=n_cols, nrows=n_rows, figsize=(width,height), constrained_layout=False, \n",
    "                                 sharex=True, sharey=False, squeeze=False)\n",
    "                                 \n",
    "        for col_i, metric in enumerate(METRICS):\n",
    "            for row_i, algo in enumerate(ALGORITHMS):\n",
    "                    # Transform strings to labels\n",
    "                    m_label = le[\"m\"].transform(np.array(metric).ravel())\n",
    "                    a_label = le[\"a\"].transform(np.array(algo).ravel())\n",
    "\n",
    "                    # Subset data and get the distances for the chosen transformation\n",
    "                    _X = df_d.query(f\"algo=={a_label} and metric == {m_label}\")\n",
    "                    \n",
    "\n",
    "                    kde_ax(_X, transform, label_encoding=le, fill=True, title=f\"{algo}-{metric}\", ax=axes[row_i, col_i])\n",
    "        fig.suptitle(f\"Inter/Intra-Score KDE Plots - {transform}\")\n",
    "        \n",
    "\n",
    "### EER multi plot\n",
    "def eer_plot_multi(transform, width=8, height=6):\n",
    "    if transform != 'Select':\n",
    "\n",
    "        n_cols = len(METRICS)\n",
    "        n_rows = len(ALGORITHMS)\n",
    "        # Subset data\n",
    "        fig, axes = plt.subplots(ncols=n_cols, nrows=n_rows, figsize=(width, height), constrained_layout=True, \n",
    "                                 sharex=True, sharey=False, squeeze=False)\n",
    "                                 \n",
    "        for col_i, metric in enumerate(METRICS):\n",
    "            for row_i, algo in enumerate(ALGORITHMS):\n",
    "                    # Transform strings to labels\n",
    "                    m_label = le[\"m\"].transform(np.array(metric).ravel())\n",
    "                    a_label = le[\"a\"].transform(np.array(algo).ravel())\n",
    "\n",
    "                    # Subset data and get the distances for the chosen transformation\n",
    "                    _X = df_d.query(f\"algo=={a_label} and metric == {m_label}\")\n",
    "\n",
    "                    # get similarities and true class labels\n",
    "                    y_true = _X[\"class\"]\n",
    "                    y_similarity = _X[transform]\n",
    "\n",
    "                    # Prepare metrics for plotting EER and AUC\n",
    "                    mm = MetricMaker(y_true=y_true, y_similarity=y_similarity, weighted=False)\n",
    "                    \n",
    "                    # Set threshold\n",
    "                    threshold = mm.eer_thresh\n",
    "\n",
    "                    # Make predictions and compute cm using EER\n",
    "                    eer_ax(mm.fpr, mm.tpr, mm.thresholds, threshold=threshold, legend=f\"\", title=f\"{algo}-{metric}\", ax=axes[row_i, col_i])\n",
    "        fig.suptitle(f\"EER Plots - {transform}\")\n",
    "        \n",
    "\n",
    "### ROC multi plot\n",
    "def roc_plot_multi(transform, width=8, height=6):\n",
    "    if transform != 'Select':\n",
    "\n",
    "        n_cols = len(METRICS)\n",
    "        n_rows = len(ALGORITHMS)\n",
    "        # Subset data\n",
    "        fig, axes = plt.subplots(ncols=n_cols, nrows=n_rows, figsize=(width,height), constrained_layout=True, \n",
    "                                 sharex=True, sharey=False, squeeze=False)\n",
    "                                 \n",
    "        for col_i, metric in enumerate(METRICS):\n",
    "            for row_i, algo in enumerate(ALGORITHMS):\n",
    "                    # Transform strings to labels\n",
    "                    m_label = le[\"m\"].transform(np.array(metric).ravel())\n",
    "                    a_label = le[\"a\"].transform(np.array(algo).ravel())\n",
    "\n",
    "                    # Subset data and get the distances for the chosen transformation\n",
    "                    _X = df_d.query(f\"algo=={a_label} and metric == {m_label}\")\n",
    "\n",
    "                    # get similarities and true class labels\n",
    "                    y_true = _X[\"class\"]\n",
    "                    y_similarity = _X[transform]\n",
    "\n",
    "                    # Prepare metrics for plotting EER and AUC\n",
    "                    mm = MetricMaker(y_true=y_true, y_similarity=y_similarity, weighted=False)\n",
    "                    \n",
    "\n",
    "\n",
    "                    # Make predictions and compute cm using EER\n",
    "                    roc_ax(mm.fpr, mm.tpr, mm.auc, title=f\"{algo}-{metric}\", ax=axes[row_i, col_i])\n",
    "        fig.suptitle(f\"ROC Plots - {transform}\")\n",
    "        \n",
    "        \n",
    "def kde_plot(transform, algorithm, metric, mode, width=8, height=6):\n",
    "    if transform != 'Select':\n",
    "        m_label = le[\"m\"].transform(np.array(metric).ravel())\n",
    "        a_label = le[\"a\"].transform(np.array(algorithm).ravel())\n",
    "        t_label = le[\"t\"].transform(np.array(transform).ravel())\n",
    "        if mode == \"inter\":\n",
    "            _X = inter_df.query(f\"algo=={a_label} and metric == {m_label}\")[transform].values\n",
    "        else:\n",
    "            _X = intra_df.query(f\"algo=={a_label} and metric == {m_label}\")[transform].values\n",
    "        sns.kdeplot(_X, fill=True)\n",
    "        plt.title(f\"Similarity Histograms - {transform} - {algorithm} - {metric}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity Score Histograms\n",
    "\n",
    "- Normalised counts of scores for each hash/transform\n",
    "- Same data as the KDE plots, but allows for a better understanding of any gaps in the distributions.\n",
    "- Updates as long as the number of bins is over 1.\n",
    "- Ideally: Inter distribution is normally distributed around 0.5, while the intra similarity is as high as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity Score Histograms\n",
    "h1 = interactive(kde_plot, transform=tselect, algorithm=aselect, metric=mselect, mode=modeselect)  # optional: save_location\n",
    "display(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity Score Histograms\n",
    "h = interactive(plot_image, transform=tselect, mode=modeselect)  # optional: save_location\n",
    "display(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Density Estimation (KDE)\n",
    "\n",
    "- Combined plot for Inter/Intra scores.\n",
    "- More or less the same as Histograms, but estimates probability density.\n",
    "- Ideally, both classes should be completely non-overlapping. Overlap is expected for difficult transforms and indicates difficulty in setting a threshold to separate them, resulting in False Positives / False Negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity Kernel Density Estimation (KDE) for inter/intra classes\n",
    "k = interactive(kde_plot_multi, transform=tselect_no_orig)\n",
    "display(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Rate\n",
    "\n",
    "- Visualise the False Positive Rate (FPR) and False Negative Rate (FNR) trade-offs across the similarity score spectrum.\n",
    "- The vertical line represents the score at which the where FPR == FNR, i.e. The Equal Error Rate Threshold (EERt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equal Error Rate (EER) similarity plots\n",
    "eer = interactive(eer_plot_multi, transform=tselect)\n",
    "display(eer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Receiver Operating Characteristic (ROC)\n",
    "\n",
    "- Plot TPR vs FPR to visualise trade-offs.\n",
    "- Provides Area Under the Curve (AUC) as a means of summarising overall performance. Larger AUC (up to 1.0) is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Receiver Operator Characteristic (ROC) similarity plots\n",
    "roc = interactive(roc_plot_multi, transform=tselect)\n",
    "display(roc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

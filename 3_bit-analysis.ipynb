{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitwise Hash Re-weighting\n",
    "\n",
    "This is notebook is a bit rougher, but allows for the replication of the Hash-bit-position Weighting used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" #type:ignore\n",
    "plt.style.use('dark_background') # dark_background \"default\n",
    "sns.set_context('paper')\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from demo00_conf import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phaser.utils import load_labelencoders, bin2bool\n",
    "\n",
    "# Load the label encoders\n",
    "le = load_labelencoders(filename=\"LabelEncoders.bz2\", path=\"./demo_outputs/\")\n",
    "\n",
    "TRANSFORMS = le['t'].classes_\n",
    "METRICS    = le['m']\n",
    "ALGORITHMS = le['a'].classes_\n",
    "# note METR_dict is imported from demo config.\n",
    "\n",
    "\n",
    "\n",
    "df_h = load(\"./demo_outputs/Hashes.df.bz2\")\n",
    "\n",
    "# Get the count of each transform. Should be 250k each.\n",
    "df_h.groupby(['transformation']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert binary to boolean for distance computation\n",
    "# for a in le[\"a\"].classes_:\n",
    "#     df_h[a] = df_h[a].apply(bin2bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove \"bad\" hashes that sum to 1 as this causes issues (Wavehash, largely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # nasty hack to find hashes with all bits set to False\n",
    "# mask = df_h['wave'].apply(lambda x: sum(x)) == 0\n",
    "# bad_filenames = df_h[mask]['filename'].unique()\n",
    "# print(f\"Found {len(bad_filenames)} bad filenames. Removing from main hashes\")\n",
    "\n",
    "# Find hashes that sum to 0 since they can cause issues with distance metrics\n",
    "for a in df_h.columns[2:]:\n",
    "    mask = df_h[a].apply(lambda x: sum(x)) == 0\n",
    "    bad_filenames = df_h[mask][\"filename\"].unique()\n",
    "\n",
    "    print(f\"{len(bad_filenames)} bad hashes found for {a}\")\n",
    "\n",
    "    if len(bad_filenames) > 0:\n",
    "        df_h = df_h[~df_h[\"filename\"].isin(bad_filenames)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset_size = 250_000\n",
    "\n",
    "# unique_files = sorted(df_h['filename'].unique())\n",
    "# sampled_files = np.random.choice(unique_files, subset_size, replace=False)\n",
    "\n",
    "# df_h_sub = df_h[df_h['filename'].isin(sampled_files)]\n",
    "# df_h_sub.groupby(['transformation']).count()\n",
    "# df_h = df_h_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Computing distances using the following metrics\")\n",
    "print(METR_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phaser.similarities import IntraDistance, InterDistance, find_inter_samplesize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTRA = IntraDistance(METR_dict, le, set_class=1, progress_bar=True)\n",
    "dist_intra = INTRA.fit(df_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_intra.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_intra.groupby(['algo','metric']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = find_inter_samplesize(len(df_h[\"filename\"].unique() * 1))\n",
    "print(n_samples)\n",
    "\n",
    "INTER = InterDistance(METR_dict, le, set_class=0, n_samples=n_samples, progress_bar=True)\n",
    "dist_inter = INTER.fit(df_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_inter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_inter.groupby(['algo','metric']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dist_intra['fileA'].unique())\n",
    "len(dist_inter['fileA'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d = pd.concat([dist_intra,dist_inter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the triplet combinations\n",
    "triplets = np.array(np.meshgrid(\n",
    "    ALGORITHMS, \n",
    "    [t for t in TRANSFORMS if t != 'orig'], # ignore 'orig'\n",
    "    METRICS.classes_)).T.reshape(-1,3)\n",
    "\n",
    "print(f\"Number of triplets to analyse: {len(triplets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phaser.evaluation import ComputeMetrics\n",
    "\n",
    "cm = ComputeMetrics(le, df_d, df_h, analyse_bits=True, n_jobs=1)\n",
    "m, b = cm.fit(triplets, weighted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.groupby(['Algorithm'])[['AUC','EER']].agg(['mean','std'])\n",
    "m.groupby(['Algorithm','Transform'])[['AUC','EER']].agg(['mean','std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute distances with bit weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phaser.evaluation import make_bit_weights\n",
    "bit_weights = make_bit_weights(b, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTRA_w = IntraDistance(METR_dict, le, set_class=1, bit_weights=bit_weights, progress_bar=True)\n",
    "dist_intra_w = INTRA_w.fit(df_h)\n",
    "INTER_w = InterDistance(METR_dict, le, set_class=0, n_samples=n_samples, bit_weights=bit_weights, progress_bar=True)\n",
    "dist_inter_w = INTER_w.fit(df_h)\n",
    "\n",
    "df_d_w = pd.concat([dist_intra_w, dist_inter_w])\n",
    "cm_w = ComputeMetrics(le, df_d_w, df_h, analyse_bits=False, n_jobs=1)\n",
    "m_w, b_w = cm_w.fit(triplets, weighted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Performance without bit weights\")\n",
    "print(m.groupby(['Algorithm', 'Metric'])[['AUC','EER']].agg(['mean','std']).to_latex(index=False))\n",
    "\n",
    "print(f\"Performance WITH bit weights\")\n",
    "print(m_w.groupby(['Algorithm', 'Metric'])[['AUC','EER']].agg(['mean','std']).to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Algorithm', 'Transform', 'Metric', 'AUC_noW', 'AUC_wW']\n",
    "_df = pd.DataFrame(np.column_stack([\n",
    "    m['Algorithm'], \n",
    "    m['Transform'], \n",
    "    m['Metric'],\n",
    "    m['AUC'],\n",
    "    m_w['AUC']]), columns=cols)\n",
    "\n",
    "_df['Improvement'] = _df['AUC_wW'] - _df['AUC_noW']\n",
    "\n",
    "plt.style.use('default') # dark_background \"default\n",
    "sns.set_context('paper')\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=FIGSIZE, constrained_layout=True)\n",
    "ax = sns.barplot(_df[_df['Metric']=='Hamming'], x='Algorithm', y='Improvement', hue='Transform', palette='Set2', ax=ax)\n",
    "# Get custom SNS legend handles from KDE plot\n",
    "handles = ax.legend_.legend_handles #type:ignore\n",
    "\n",
    "for handle, txt in zip(handles, ax.legend_.texts): #type:ignore\n",
    "    # assign the legend labels to the handles\n",
    "    handle.set_label(txt.get_text().split(\"_\")[0]) #type:ignore\n",
    "# Update custom SNS legend with the added line.\n",
    "_ = ax.legend(handles=handles , loc=\"upper right\", title='Transform')\n",
    "_ = ax.grid(axis='y', alpha=0.35)\n",
    "\n",
    "fig.savefig(\"./demo_outputs/figs/AUC_weight_improvements.pdf\")  \n",
    "\n",
    "m_w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising bit weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phaser.plotting import bit_weights_ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(5,1.5), constrained_layout=True)\n",
    "_ = bit_weights_ax(b[\"phash_Flip_Horizontal_Hamming\"], ax=ax)\n",
    "fig.savefig(\"./demo_outputs/figs/bit_w_phash_Flip_Horizontal_Hamming.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[k for k in bit_weights.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(5,1.5), constrained_layout=True)\n",
    "_ = bit_weights_ax(pd.DataFrame(bit_weights['phash_Hamming']), ax=ax)\n",
    "fig.savefig(\"./demo_outputs/figs/bit_w_phash_median.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_lib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Pre-requisite Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "# Note: On Windows some packages require Visual Studio Code (with the Windows 10/11 SDK) to build."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Available Modules\n",
    "Run the Cell below to list the names of the available:\n",
    "- Perceptual Hashing Algorithms\n",
    "- Transformers\n",
    "- Similarity Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooksupport import list_modular_components\n",
    "\n",
    "nl = '\\n'\n",
    "for module_name, functions in list_modular_components().items():\n",
    "    print( f\"{module_name}:{nl}{nl.join(functions)}\")\n",
    "    print(nl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Transforms\n",
    "- Test out transforms and their parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phaser.transformers import *\n",
    "from phaser.utils import ImageLoader\n",
    "\n",
    "# list of functions and parameters to create transform classes\n",
    "# Edit these to change output.\n",
    "transforms = [\n",
    "    Blend(direction=\"left-to-right\", static_image=\"phaser/resources/high_freq_grass.jpg\"),\n",
    "    Border(border_colour=(255,255,255), border_width=30, name=\"border-30px-white\"),\n",
    "    Border(border_colour=(255,255,255), border_width_fraction=1/16, name=\"border-frac-white\"),\n",
    "    Composite(position=\"top-left\", scale=True, static_image=\"phaser/resources/high_freq_grass.jpg\", name=\"composite_top-left\"),\n",
    "    Composite(position=\"right\", scale=True, static_image=\"phaser/resources/high_freq_grass.jpg\", name=\"composite_right\"),\n",
    "    Crop(cropbox_absolute=[20,20,0,0]),\n",
    "    Crop(cropbox_factors=[.25,.25,.0,.0], name=\"Crop_factors_25_25_0_0\"), # 25% from left and top edges.\n",
    "    Enhance(sharpnessfactor=0.0, contrastfactor=2, colourfactor=0.3, brightnessfactor=0.7),\n",
    "    Flip(direction='Horizontal'),\n",
    "    Rescale(scalefactor=2.1, thumbnail_aspect=False),\n",
    "    Rescale(fixed_dimensions=(200,200), thumbnail_aspect=True),\n",
    "    Rotate(degrees_counter_clockwise=15),\n",
    "    Watermark()\n",
    "]\n",
    "\n",
    "# Load an image to transform\n",
    "im = ImageLoader(f\"./images/17273391_55cfc7d3d4.jpg\")\n",
    "\n",
    "# Apply each specified transform to the image and save to \"test/\"\n",
    "for t in transforms:\n",
    "    modified = t.fit(im) # fit function applies the transform\n",
    "    filepath = t.saveToDisk(image=modified, save_directory=\"test\", filename=im.filename )\n",
    "    print(filepath)\n",
    "    display(modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Experimental Conditions\n",
    "- Specify path to original (unmodified) images. Transforms will be applied to these, and these serve as the ground truth for intra-distance analysis.\n",
    "- Specify output directory\n",
    "- Specify dictionary of Perceptual Hashes to use (with parameters)\n",
    "    - Format: ```<name_string>: <class_name>(<arguments>)```\n",
    "    - Must be imported in this cell, base algorithms defined in ```phaser.hashing._algorithms.py```.\n",
    "    - Class should extend ```phaser.hashing._algorithms.PerceptalHash```\n",
    "- Specify Transforms to use.\n",
    "    - Format: ```<clas_name>(<arguments>)```\n",
    "    - Must be imported in this cell, base algorithms defined in ```phaser.transformers._transforms.py```.\n",
    "    - Use ```TransformFromDisk``` and specify a path as the argument if the transform files already exist on disk (must have same name as originals to match)\n",
    "- Specify Distance metrics to use.\n",
    "    - Format: Dictionary with ```<Human Readable Name>:<metric>``` (metric should be str or Callable, see below)\n",
    "    - If the distance metric is part of ```scipy.spatial.distance```, specify the name as a ```str```\n",
    "    - If a custom distance metric is provided in ```phaser.similarities._distances.py```, import and pass the ```function reference```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify path of the original (non-transformed) dataset\n",
    "# e.g. \"F:\\Datasets\\images\"\n",
    "original_path = \"images\"\n",
    "\n",
    "# Specify output directory\n",
    "output_directory = r\"demo_outputs\"\n",
    "\n",
    "# Specify Perceptual Hashing Algorithms\n",
    "\n",
    "from phaser.hashing import *\n",
    "ALGORITHMS = {\n",
    "        \"ahash\": AverageHash(),\n",
    "        #'Colourhash': ColourHash(), # Probably to be avoided, generaly.\n",
    "        \"dhash\": DifferenceHash(),\n",
    "        \"dhash_vertical\": DifferenceHash(vertical=True),\n",
    "        \"phash\": PHash(hash_size=8), \n",
    "        \"wave\": WaveHash(), \n",
    "        \"pdq\": PdqHash()\n",
    "        }\n",
    "\n",
    "# Specify Transforms functions \n",
    "# Transforms can be saved (saveToPath) for later if desired.\n",
    "from phaser.transformers import *\n",
    "TRANSFORMERS = [\n",
    "    Border(border_colour=(255,255,255), border_width=30),\n",
    "    Crop(cropbox_factors=[.05,.05,.05,.05]),\n",
    "    Flip(direction='Horizontal'),\n",
    "    Rescale(fixed_dimensions=(96,96), thumbnail_aspect=True),\n",
    "    Rotate(degrees_counter_clockwise=5),\n",
    "    Watermark()\n",
    "    # TransformFromDisk(r\"F:/testmods/Border30black\"), # example loading from disk\n",
    "    ]\n",
    "\n",
    "from phaser.similarities import *\n",
    "# import numpy as np  #import if specifying filters to Convolution Distance. Otherwise not needed here.\n",
    "from scipy.spatial.distance import hamming # passed to Hatched Matrix distance to demonstrate it can be changed.\n",
    "DISTANCE_METRICS = {\n",
    "    \"Hamming\": \"hamming\",\n",
    "    \"Cosine\": \"cosine\",\n",
    "    \"Convolution_Sumdiff\": (convolution_distance, {\"mode\":\"sum_diffs\"}),\n",
    "    \"Hatched_Matrix\": (hatched_matrix2, {\"distance_fun\": hamming}),\n",
    "    # \"2gram_Cosine\": (ngram_cosine_distance, {\"ngram_size\": 2}),\n",
    "    # \"Test_Synthetic\": test_synthetic\n",
    "}\n",
    "\n",
    "# === EDIT ABOVE LINE ======================\n",
    "\n",
    "# Test that metrics have been entered correctly\n",
    "from phaser.similarities import validate_metrics\n",
    "if validate_metrics(DISTANCE_METRICS):\n",
    "    print(\"Metrics look valid!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Files\n",
    "\n",
    "- Hash original files with each algorithm\n",
    "- Generate and hash transform files\n",
    "- Output hashes to CSV files compressed to .bz (bzip) files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooksupport import do_hashing\n",
    "\n",
    "# Pass the settings through to a helper function which does the hashing and applies transforms\n",
    "do_hashing(originals_path=original_path, output_directory=output_directory, \n",
    "           algorithms=ALGORITHMS, transformers=TRANSFORMERS, \n",
    "           n_jobs=12, backend=\"threading\", progress_report=True, batch_size=100_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Similarity Scores\n",
    "There are two types of score calculated here:\n",
    "- Intra-score: Where the original images are compared to their modifications. This is a 1-to-1 mapping (N * #hashes * #transforms * #comparison_metrics)\n",
    "    - This is used to determine how robust the hash and comparison metric are to each transform class.\n",
    "    - Ideally original images should have a distance of 0 (similarity of 1) to their transforms.\n",
    "- Inter-score: Where images within a given transform (or originals) are compared to themselves for each given comparison metric. Inter-scores are sampled to match the size of the intra-score class. (Calculating all pairwise combinations generates many more samples, (N*N-1/2)\n",
    "    - This gives us a baseline behaviour, with the assumption that the images should *not* match.\n",
    "    - On aggregate, random unrelated images should be about 0.5 different, as this makes the best use of the metric space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooksupport import calculate_distances\n",
    "\n",
    "# Load hashes and labels from the output generated by the previous step and calculate inter- and intra-distances.\n",
    "calculate_distances(hash_directory=output_directory, distance_metrics=DISTANCE_METRICS, progress_report=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
